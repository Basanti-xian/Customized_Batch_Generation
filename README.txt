Classification 1:
    Classified the whole bags, including different color.

    --Model:
        1 layer CNN model(filter_size =7, filter_number =3)

    --Preprocess: (data_process.py)
        crop image.
        box(0, 217, 945, 625), just remove the text

    --Training and Test: (classification.py)
        Input: cropped image  & label

    --Result:
        Accuracy on Test-Set: 60.0% (3 / 5)
                      precision    recall  f1-score   support

                   0       1.00      0.50      0.67         2
                   1       0.00      0.00      0.00         0
                   2       0.67      0.67      0.67         3

    --What's more?
        *The result is not accurate at all, because there is only 5 samples used for test.
        *The model may over-fitting, but data augmentation is not suitable here.
        *In my opinion, brightness and color are very important feature which cannot be changed, while rotation angle are consistent that no need to change.


Classification 2:
    To classify the details part. Details are cropped from the oringinal images ramdomly.
    During training, every input batch is generated by randomly cropped images into subimages.
    As for test, images would also be cropped. Then, all of the cropped patches would be participated into voting for final prediction of the whole image.

    --Model:
        Pretrained VGG16 model + 1 dense layer

    --Preprocess: (data_process.py)
        crop image.
        box(0, 217, 945, 625), just remove the text

    --Training: (vgg16_classification.py)
        Input: cropped image  & label

    --Test:(evaluation.py)
        Input: cropped image  & label

    --Result:
        Accuracy:  0.6

    --What's more?
        *How to get the details patch? The maximum and minimum width and height are ramdomly generate within a range.Then, the patch would be resized to 224x224.
	 This crop method can be improved, if we do a object detection first to find out the bags location. (need more data to do object detection!!!)
	 Then we can seperate the background with bags. Background didn't contain details, should not be cropped.

        *Every input batch is generated by randomly cropped images into subimages. So, we artificially generate new data for every batch.
        *The result is not accurate at all, because there is only 5 samples used for test.
        *The result can be much better, if we use more data and try with different model.




Classification 3:
    Using Secne text detection to get the text from raw images. Then, build a simple SVM model to classify those text into categories.
    (Because of the data limitation, it has not been implemented.)



** Concatenante these 3 models. Using another majority voting to give us the final result.
